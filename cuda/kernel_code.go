//+build !nocuda

package cuda

var kernelPTX = `
//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21313162
// Cuda compilation tools, release 8.0, V8.0.53
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_30
.address_size 64

	// .globl	divElements
.extern .shared .align 4 .b8 chunk[];
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry divElements(
	.param .u64 divElements_param_0,
	.param .u64 divElements_param_1,
	.param .u32 divElements_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [divElements_param_0];
	ld.param.u64 	%rd2, [divElements_param_1];
	ld.param.u32 	%r2, [divElements_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB0_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	ld.global.f32 	%f1, [%rd7];
	ld.global.f32 	%f2, [%rd5];
	div.rn.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd5], %f3;

BB0_2:
	ret;
}

	// .globl	expElements
.visible .entry expElements(
	.param .u64 expElements_param_0,
	.param .u32 expElements_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [expElements_param_0];
	ld.param.u32 	%r2, [expElements_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB1_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f3, [%rd4];
	mul.f32 	%f4, %f3, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f5, %f4;
	mov.f32 	%f6, 0fBF317200;
	fma.rn.f32 	%f7, %f5, %f6, %f3;
	mov.f32 	%f8, 0fB5BFBE8E;
	fma.rn.f32 	%f9, %f5, %f8, %f7;
	mul.f32 	%f2, %f9, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f1,%f2;
	// inline asm
	add.f32 	%f10, %f5, 0f00000000;
	ex2.approx.f32 	%f11, %f10;
	mul.f32 	%f12, %f1, %f11;
	setp.lt.f32	%p2, %f3, 0fC2D20000;
	selp.f32	%f13, 0f00000000, %f12, %p2;
	setp.gt.f32	%p3, %f3, 0f42D20000;
	selp.f32	%f14, 0f7F800000, %f13, %p3;
	st.global.f32 	[%rd4], %f14;

BB1_2:
	ret;
}

	// .globl	logElements
.visible .entry logElements(
	.param .u64 logElements_param_0,
	.param .u32 logElements_param_1
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [logElements_param_0];
	ld.param.u32 	%r2, [logElements_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB2_4;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f5, [%rd1];
	setp.lt.f32	%p2, %f5, 0f00800000;
	mul.f32 	%f6, %f5, 0f4B000000;
	selp.f32	%f1, %f6, %f5, %p2;
	selp.f32	%f7, 0fC1B80000, 0f00000000, %p2;
	mov.b32 	 %r6, %f1;
	add.s32 	%r7, %r6, -1059760811;
	and.b32  	%r8, %r7, -8388608;
	sub.s32 	%r9, %r6, %r8;
	mov.b32 	 %f8, %r9;
	cvt.rn.f32.s32	%f9, %r8;
	mov.f32 	%f10, 0f34000000;
	fma.rn.f32 	%f11, %f9, %f10, %f7;
	add.f32 	%f12, %f8, 0fBF800000;
	mov.f32 	%f13, 0f3E1039F6;
	mov.f32 	%f14, 0fBE055027;
	fma.rn.f32 	%f15, %f14, %f12, %f13;
	mov.f32 	%f16, 0fBDF8CDCC;
	fma.rn.f32 	%f17, %f15, %f12, %f16;
	mov.f32 	%f18, 0f3E0F2955;
	fma.rn.f32 	%f19, %f17, %f12, %f18;
	mov.f32 	%f20, 0fBE2AD8B9;
	fma.rn.f32 	%f21, %f19, %f12, %f20;
	mov.f32 	%f22, 0f3E4CED0B;
	fma.rn.f32 	%f23, %f21, %f12, %f22;
	mov.f32 	%f24, 0fBE7FFF22;
	fma.rn.f32 	%f25, %f23, %f12, %f24;
	mov.f32 	%f26, 0f3EAAAA78;
	fma.rn.f32 	%f27, %f25, %f12, %f26;
	mov.f32 	%f28, 0fBF000000;
	fma.rn.f32 	%f29, %f27, %f12, %f28;
	mul.f32 	%f30, %f12, %f29;
	fma.rn.f32 	%f31, %f30, %f12, %f12;
	mov.f32 	%f32, 0f3F317218;
	fma.rn.f32 	%f35, %f11, %f32, %f31;
	setp.lt.u32	%p3, %r6, 2139095040;
	@%p3 bra 	BB2_3;

	mov.f32 	%f33, 0f7F800000;
	fma.rn.f32 	%f35, %f1, %f33, %f33;

BB2_3:
	setp.eq.f32	%p4, %f1, 0f00000000;
	selp.f32	%f34, 0fFF800000, %f35, %p4;
	st.global.f32 	[%rd1], %f34;

BB2_4:
	ret;
}

	// .globl	tanhElements
.visible .entry tanhElements(
	.param .u64 tanhElements_param_0,
	.param .u32 tanhElements_param_1
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<33>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [tanhElements_param_0];
	ld.param.u32 	%r2, [tanhElements_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB3_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd1];
	abs.f32 	%f2, %f1;
	setp.ltu.f32	%p2, %f2, 0f3F0CCCCD;
	@%p2 bra 	BB3_3;
	bra.uni 	BB3_2;

BB3_3:
	mul.f32 	%f21, %f1, %f1;
	mov.f32 	%f22, 0fBD57BE66;
	mov.f32 	%f23, 0f3C86A81B;
	fma.rn.f32 	%f24, %f23, %f21, %f22;
	mov.f32 	%f25, 0f3E08677B;
	fma.rn.f32 	%f26, %f24, %f21, %f25;
	mov.f32 	%f27, 0fBEAAAA29;
	fma.rn.f32 	%f28, %f26, %f21, %f27;
	mul.f32 	%f29, %f21, %f28;
	fma.rn.f32 	%f30, %f29, %f1, %f1;
	add.f32 	%f31, %f1, %f1;
	setp.eq.f32	%p4, %f1, 0f00000000;
	selp.f32	%f32, %f31, %f30, %p4;
	bra.uni 	BB3_4;

BB3_2:
	add.f32 	%f10, %f2, %f2;
	mul.f32 	%f11, %f10, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f12, %f11;
	mov.f32 	%f13, 0fBF317200;
	fma.rn.f32 	%f14, %f12, %f13, %f10;
	mov.f32 	%f15, 0fB5BFBE8E;
	fma.rn.f32 	%f16, %f12, %f15, %f14;
	mul.f32 	%f7, %f16, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f6,%f7;
	// inline asm
	ex2.approx.f32 	%f17, %f12;
	mov.f32 	%f18, 0f3F800000;
	fma.rn.f32 	%f9, %f6, %f17, %f18;
	// inline asm
	rcp.approx.ftz.f32 %f8,%f9;
	// inline asm
	mov.f32 	%f19, 0fC0000000;
	fma.rn.f32 	%f20, %f8, %f19, %f18;
	mov.b32 	 %r6, %f20;
	setp.ltu.f32	%p3, %f2, 0f42B00000;
	selp.b32	%r7, %r6, 1065353216, %p3;
	mov.b32 	 %r8, %f1;
	and.b32  	%r9, %r8, -2147483648;
	or.b32  	%r10, %r7, %r9;
	mov.b32 	 %f32, %r10;

BB3_4:
	st.global.f32 	[%rd1], %f32;

BB3_5:
	ret;
}

	// .globl	sinElements
.visible .entry sinElements(
	.param .u64 sinElements_param_0,
	.param .u32 sinElements_param_1
)
{
	.local .align 4 .b8 	__local_depot4[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .f32 	%f<48>;
	.reg .b32 	%r<97>;
	.reg .b64 	%rd<17>;


	mov.u64 	%rd16, __local_depot4;
	cvta.local.u64 	%SP, %rd16;
	ld.param.u64 	%rd8, [sinElements_param_0];
	ld.param.u32 	%r37, [sinElements_param_1];
	mov.u32 	%r38, %ntid.x;
	mov.u32 	%r39, %ctaid.x;
	mov.u32 	%r40, %tid.x;
	mad.lo.s32 	%r1, %r38, %r39, %r40;
	setp.ge.s32	%p1, %r1, %r37;
	@%p1 bra 	BB4_24;

	cvta.to.global.u64 	%rd9, %rd8;
	mul.wide.s32 	%rd10, %r1, 4;
	add.s64 	%rd1, %rd9, %rd10;
	add.u64 	%rd11, %SP, 0;
	cvta.to.local.u64 	%rd2, %rd11;
	ld.global.f32 	%f43, [%rd1];
	abs.f32 	%f19, %f43;
	setp.neu.f32	%p2, %f19, 0f7F800000;
	@%p2 bra 	BB4_3;

	mov.f32 	%f20, 0f00000000;
	mul.rn.f32 	%f43, %f43, %f20;

BB4_3:
	mul.f32 	%f21, %f43, 0f3F22F983;
	cvt.rni.s32.f32	%r96, %f21;
	cvt.rn.f32.s32	%f22, %r96;
	neg.f32 	%f23, %f22;
	mov.f32 	%f24, 0f3FC90FDA;
	fma.rn.f32 	%f25, %f23, %f24, %f43;
	mov.f32 	%f26, 0f33A22168;
	fma.rn.f32 	%f27, %f23, %f26, %f25;
	mov.f32 	%f28, 0f27C234C5;
	fma.rn.f32 	%f44, %f23, %f28, %f27;
	abs.f32 	%f29, %f43;
	setp.leu.f32	%p3, %f29, 0f47CE4780;
	@%p3 bra 	BB4_13;

	mov.b32 	 %r3, %f43;
	shr.u32 	%r4, %r3, 23;
	shl.b32 	%r43, %r3, 8;
	or.b32  	%r5, %r43, -2147483648;
	mov.u32 	%r88, 0;
	mov.u64 	%rd14, __cudart_i2opi_f;
	mov.u32 	%r87, -6;
	mov.u64 	%rd15, %rd2;

BB4_5:
	.pragma "nounroll";
	mov.u64 	%rd4, %rd15;
	ld.const.u32 	%r46, [%rd14];
	// inline asm
	{
	mad.lo.cc.u32   %r44, %r46, %r5, %r88;
	madc.hi.u32     %r88, %r46, %r5,  0;
	}
	// inline asm
	st.local.u32 	[%rd4], %r44;
	add.s64 	%rd5, %rd4, 4;
	add.s64 	%rd14, %rd14, 4;
	add.s32 	%r87, %r87, 1;
	setp.ne.s32	%p4, %r87, 0;
	mov.u64 	%rd15, %rd5;
	@%p4 bra 	BB4_5;

	and.b32  	%r49, %r4, 255;
	add.s32 	%r50, %r49, -128;
	shr.u32 	%r51, %r50, 5;
	and.b32  	%r10, %r3, -2147483648;
	st.local.u32 	[%rd2+24], %r88;
	mov.u32 	%r52, 6;
	sub.s32 	%r53, %r52, %r51;
	mul.wide.s32 	%rd13, %r53, 4;
	add.s64 	%rd7, %rd2, %rd13;
	ld.local.u32 	%r89, [%rd7];
	ld.local.u32 	%r90, [%rd7+-4];
	and.b32  	%r13, %r4, 31;
	setp.eq.s32	%p5, %r13, 0;
	@%p5 bra 	BB4_8;

	mov.u32 	%r54, 32;
	sub.s32 	%r55, %r54, %r13;
	shr.u32 	%r56, %r90, %r55;
	shl.b32 	%r57, %r89, %r13;
	add.s32 	%r89, %r56, %r57;
	ld.local.u32 	%r58, [%rd7+-8];
	shr.u32 	%r59, %r58, %r55;
	shl.b32 	%r60, %r90, %r13;
	add.s32 	%r90, %r59, %r60;

BB4_8:
	shr.u32 	%r61, %r90, 30;
	shl.b32 	%r62, %r89, 2;
	add.s32 	%r91, %r61, %r62;
	shl.b32 	%r19, %r90, 2;
	shr.u32 	%r63, %r91, 31;
	shr.u32 	%r64, %r89, 30;
	add.s32 	%r20, %r63, %r64;
	setp.eq.s32	%p6, %r63, 0;
	mov.u32 	%r92, %r10;
	mov.u32 	%r93, %r19;
	@%p6 bra 	BB4_10;

	not.b32 	%r65, %r91;
	neg.s32 	%r21, %r19;
	setp.eq.s32	%p7, %r19, 0;
	selp.u32	%r66, 1, 0, %p7;
	add.s32 	%r91, %r66, %r65;
	xor.b32  	%r23, %r10, -2147483648;
	mov.u32 	%r92, %r23;
	mov.u32 	%r93, %r21;

BB4_10:
	mov.u32 	%r25, %r92;
	neg.s32 	%r67, %r20;
	setp.eq.s32	%p8, %r10, 0;
	selp.b32	%r96, %r20, %r67, %p8;
	clz.b32 	%r95, %r91;
	setp.eq.s32	%p9, %r95, 0;
	shl.b32 	%r68, %r91, %r95;
	mov.u32 	%r69, 32;
	sub.s32 	%r70, %r69, %r95;
	shr.u32 	%r71, %r93, %r70;
	add.s32 	%r72, %r71, %r68;
	selp.b32	%r29, %r91, %r72, %p9;
	mov.u32 	%r73, -921707870;
	mul.hi.u32 	%r94, %r29, %r73;
	setp.lt.s32	%p10, %r94, 1;
	@%p10 bra 	BB4_12;

	mul.lo.s32 	%r74, %r29, -921707870;
	shr.u32 	%r75, %r74, 31;
	shl.b32 	%r76, %r94, 1;
	add.s32 	%r94, %r75, %r76;
	add.s32 	%r95, %r95, 1;

BB4_12:
	mov.u32 	%r77, 126;
	sub.s32 	%r78, %r77, %r95;
	shl.b32 	%r79, %r78, 23;
	add.s32 	%r80, %r94, 1;
	shr.u32 	%r81, %r80, 7;
	add.s32 	%r82, %r81, 1;
	shr.u32 	%r83, %r82, 1;
	add.s32 	%r84, %r83, %r79;
	or.b32  	%r85, %r84, %r25;
	mov.b32 	 %f44, %r85;

BB4_13:
	mul.rn.f32 	%f7, %f44, %f44;
	and.b32  	%r36, %r96, 1;
	setp.eq.s32	%p11, %r36, 0;
	@%p11 bra 	BB4_15;

	mov.f32 	%f30, 0fBAB6061A;
	mov.f32 	%f31, 0f37CCF5CE;
	fma.rn.f32 	%f45, %f31, %f7, %f30;
	bra.uni 	BB4_16;

BB4_15:
	mov.f32 	%f32, 0f3C08839E;
	mov.f32 	%f33, 0fB94CA1F9;
	fma.rn.f32 	%f45, %f33, %f7, %f32;

BB4_16:
	@%p11 bra 	BB4_18;

	mov.f32 	%f34, 0f3D2AAAA5;
	fma.rn.f32 	%f35, %f45, %f7, %f34;
	mov.f32 	%f36, 0fBF000000;
	fma.rn.f32 	%f46, %f35, %f7, %f36;
	bra.uni 	BB4_19;

BB4_18:
	mov.f32 	%f37, 0fBE2AAAA3;
	fma.rn.f32 	%f38, %f45, %f7, %f37;
	mov.f32 	%f39, 0f00000000;
	fma.rn.f32 	%f46, %f38, %f7, %f39;

BB4_19:
	fma.rn.f32 	%f47, %f46, %f44, %f44;
	@%p11 bra 	BB4_21;

	mov.f32 	%f40, 0f3F800000;
	fma.rn.f32 	%f47, %f46, %f7, %f40;

BB4_21:
	and.b32  	%r86, %r96, 2;
	setp.eq.s32	%p14, %r86, 0;
	@%p14 bra 	BB4_23;

	mov.f32 	%f41, 0f00000000;
	mov.f32 	%f42, 0fBF800000;
	fma.rn.f32 	%f47, %f47, %f42, %f41;

BB4_23:
	st.global.f32 	[%rd1], %f47;

BB4_24:
	ret;
}

	// .globl	sigmoidElements
.visible .entry sigmoidElements(
	.param .u64 sigmoidElements_param_0,
	.param .u32 sigmoidElements_param_1
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [sigmoidElements_param_0];
	ld.param.u32 	%r2, [sigmoidElements_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB5_5;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f6, [%rd1];
	mul.f32 	%f1, %f6, 0f3F000000;
	abs.f32 	%f2, %f1;
	setp.ltu.f32	%p2, %f2, 0f3F0CCCCD;
	@%p2 bra 	BB5_3;
	bra.uni 	BB5_2;

BB5_3:
	mul.f32 	%f22, %f1, %f1;
	mov.f32 	%f23, 0fBD57BE66;
	mov.f32 	%f24, 0f3C86A81B;
	fma.rn.f32 	%f25, %f24, %f22, %f23;
	mov.f32 	%f26, 0f3E08677B;
	fma.rn.f32 	%f27, %f25, %f22, %f26;
	mov.f32 	%f28, 0fBEAAAA29;
	fma.rn.f32 	%f29, %f27, %f22, %f28;
	mul.f32 	%f30, %f22, %f29;
	fma.rn.f32 	%f31, %f30, %f1, %f1;
	add.f32 	%f32, %f1, %f1;
	setp.eq.f32	%p4, %f1, 0f00000000;
	selp.f32	%f35, %f32, %f31, %p4;
	bra.uni 	BB5_4;

BB5_2:
	add.f32 	%f11, %f2, %f2;
	mul.f32 	%f12, %f11, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f13, %f12;
	mov.f32 	%f14, 0fBF317200;
	fma.rn.f32 	%f15, %f13, %f14, %f11;
	mov.f32 	%f16, 0fB5BFBE8E;
	fma.rn.f32 	%f17, %f13, %f16, %f15;
	mul.f32 	%f8, %f17, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f7,%f8;
	// inline asm
	ex2.approx.f32 	%f18, %f13;
	mov.f32 	%f19, 0f3F800000;
	fma.rn.f32 	%f10, %f7, %f18, %f19;
	// inline asm
	rcp.approx.ftz.f32 %f9,%f10;
	// inline asm
	mov.f32 	%f20, 0fC0000000;
	fma.rn.f32 	%f21, %f9, %f20, %f19;
	mov.b32 	 %r6, %f21;
	setp.ltu.f32	%p3, %f2, 0f42B00000;
	selp.b32	%r7, %r6, 1065353216, %p3;
	mov.b32 	 %r8, %f1;
	and.b32  	%r9, %r8, -2147483648;
	or.b32  	%r10, %r7, %r9;
	mov.b32 	 %f35, %r10;

BB5_4:
	add.f32 	%f33, %f35, 0f3F800000;
	mul.f32 	%f34, %f33, 0f3F000000;
	st.global.f32 	[%rd1], %f34;

BB5_5:
	ret;
}

	// .globl	clipPositive
.visible .entry clipPositive(
	.param .u64 clipPositive_param_0,
	.param .u32 clipPositive_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [clipPositive_param_0];
	ld.param.u32 	%r2, [clipPositive_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB6_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f1, [%rd4];
	mov.f32 	%f2, 0f00000000;
	max.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

BB6_2:
	ret;
}

	// .globl	shiftRandUniform
.visible .entry shiftRandUniform(
	.param .u64 shiftRandUniform_param_0,
	.param .u32 shiftRandUniform_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [shiftRandUniform_param_0];
	ld.param.u32 	%r2, [shiftRandUniform_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB7_3;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd1];
	setp.neu.f32	%p2, %f1, 0f3F800000;
	@%p2 bra 	BB7_3;

	mov.u32 	%r6, 0;
	st.global.u32 	[%rd1], %r6;

BB7_3:
	ret;
}

	// .globl	uniformToBernoulli
.visible .entry uniformToBernoulli(
	.param .u64 uniformToBernoulli_param_0,
	.param .u32 uniformToBernoulli_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [uniformToBernoulli_param_0];
	ld.param.u32 	%r2, [uniformToBernoulli_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB8_4;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd1];
	setp.gt.f32	%p2, %f1, 0f3F000000;
	@%p2 bra 	BB8_3;
	bra.uni 	BB8_2;

BB8_3:
	mov.u32 	%r7, 1065353216;
	st.global.u32 	[%rd1], %r7;
	bra.uni 	BB8_4;

BB8_2:
	mov.u32 	%r6, 0;
	st.global.u32 	[%rd1], %r6;

BB8_4:
	ret;
}

	// .globl	addRepeated
.visible .entry addRepeated(
	.param .u64 addRepeated_param_0,
	.param .u64 addRepeated_param_1,
	.param .u32 addRepeated_param_2,
	.param .u32 addRepeated_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addRepeated_param_0];
	ld.param.u64 	%rd2, [addRepeated_param_1];
	ld.param.u32 	%r3, [addRepeated_param_2];
	ld.param.u32 	%r2, [addRepeated_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB9_2;

	cvta.to.global.u64 	%rd3, %rd2;
	rem.s32 	%r7, %r1, %r2;
	mul.wide.s32 	%rd4, %r7, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd5];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd8], %f3;

BB9_2:
	ret;
}

	// .globl	addRepeatedPow2
.visible .entry addRepeatedPow2(
	.param .u64 addRepeatedPow2_param_0,
	.param .u64 addRepeatedPow2_param_1,
	.param .u32 addRepeatedPow2_param_2,
	.param .u32 addRepeatedPow2_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addRepeatedPow2_param_0];
	ld.param.u64 	%rd2, [addRepeatedPow2_param_1];
	ld.param.u32 	%r3, [addRepeatedPow2_param_2];
	ld.param.u32 	%r2, [addRepeatedPow2_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB10_2;

	cvta.to.global.u64 	%rd3, %rd2;
	and.b32  	%r7, %r1, %r2;
	mul.wide.s32 	%rd4, %r7, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd5];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd8], %f3;

BB10_2:
	ret;
}

	// .globl	scaleRepeated
.visible .entry scaleRepeated(
	.param .u64 scaleRepeated_param_0,
	.param .u64 scaleRepeated_param_1,
	.param .u32 scaleRepeated_param_2,
	.param .u32 scaleRepeated_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [scaleRepeated_param_0];
	ld.param.u64 	%rd2, [scaleRepeated_param_1];
	ld.param.u32 	%r3, [scaleRepeated_param_2];
	ld.param.u32 	%r2, [scaleRepeated_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB11_2;

	cvta.to.global.u64 	%rd3, %rd2;
	rem.s32 	%r7, %r1, %r2;
	mul.wide.s32 	%rd4, %r7, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd5];
	mul.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd8], %f3;

BB11_2:
	ret;
}

	// .globl	scaleRepeatedPow2
.visible .entry scaleRepeatedPow2(
	.param .u64 scaleRepeatedPow2_param_0,
	.param .u64 scaleRepeatedPow2_param_1,
	.param .u32 scaleRepeatedPow2_param_2,
	.param .u32 scaleRepeatedPow2_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [scaleRepeatedPow2_param_0];
	ld.param.u64 	%rd2, [scaleRepeatedPow2_param_1];
	ld.param.u32 	%r3, [scaleRepeatedPow2_param_2];
	ld.param.u32 	%r2, [scaleRepeatedPow2_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB12_2;

	cvta.to.global.u64 	%rd3, %rd2;
	and.b32  	%r7, %r1, %r2;
	mul.wide.s32 	%rd4, %r7, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd5];
	mul.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd8], %f3;

BB12_2:
	ret;
}

	// .globl	addScaler
.visible .entry addScaler(
	.param .f32 addScaler_param_0,
	.param .u64 addScaler_param_1,
	.param .u32 addScaler_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.f32 	%f1, [addScaler_param_0];
	ld.param.u64 	%rd1, [addScaler_param_1];
	ld.param.u32 	%r2, [addScaler_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB13_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

BB13_2:
	ret;
}

	// .globl	addChunks
.visible .entry addChunks(
	.param .u64 addChunks_param_0,
	.param .u64 addChunks_param_1,
	.param .u32 addChunks_param_2,
	.param .u32 addChunks_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [addChunks_param_0];
	ld.param.u64 	%rd2, [addChunks_param_1];
	ld.param.u32 	%r3, [addChunks_param_2];
	ld.param.u32 	%r2, [addChunks_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB14_2;

	cvta.to.global.u64 	%rd3, %rd2;
	div.s32 	%r7, %r1, %r2;
	mul.wide.s32 	%rd4, %r7, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd5];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd8], %f3;

BB14_2:
	ret;
}

	// .globl	subChunks
.visible .entry subChunks(
	.param .u64 subChunks_param_0,
	.param .u64 subChunks_param_1,
	.param .u32 subChunks_param_2,
	.param .u32 subChunks_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [subChunks_param_0];
	ld.param.u64 	%rd2, [subChunks_param_1];
	ld.param.u32 	%r3, [subChunks_param_2];
	ld.param.u32 	%r2, [subChunks_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.s32	%p1, %r1, %r3;
	@%p1 bra 	BB15_2;

	cvta.to.global.u64 	%rd3, %rd2;
	div.s32 	%r7, %r1, %r2;
	mul.wide.s32 	%rd4, %r7, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8];
	ld.global.f32 	%f2, [%rd5];
	sub.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd8], %f3;

BB15_2:
	ret;
}

	// .globl	lessThan
.visible .entry lessThan(
	.param .f32 lessThan_param_0,
	.param .u64 lessThan_param_1,
	.param .u32 lessThan_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<5>;


	ld.param.f32 	%f1, [lessThan_param_0];
	ld.param.u64 	%rd2, [lessThan_param_1];
	ld.param.u32 	%r2, [lessThan_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB16_4;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd1];
	setp.lt.f32	%p2, %f2, %f1;
	@%p2 bra 	BB16_3;
	bra.uni 	BB16_2;

BB16_3:
	mov.u32 	%r7, 1065353216;
	st.global.u32 	[%rd1], %r7;
	bra.uni 	BB16_4;

BB16_2:
	mov.u32 	%r6, 0;
	st.global.u32 	[%rd1], %r6;

BB16_4:
	ret;
}

	// .globl	greaterThan
.visible .entry greaterThan(
	.param .f32 greaterThan_param_0,
	.param .u64 greaterThan_param_1,
	.param .u32 greaterThan_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<5>;


	ld.param.f32 	%f1, [greaterThan_param_0];
	ld.param.u64 	%rd2, [greaterThan_param_1];
	ld.param.u32 	%r2, [greaterThan_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB17_4;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd1];
	setp.gt.f32	%p2, %f2, %f1;
	@%p2 bra 	BB17_3;
	bra.uni 	BB17_2;

BB17_3:
	mov.u32 	%r7, 1065353216;
	st.global.u32 	[%rd1], %r7;
	bra.uni 	BB17_4;

BB17_2:
	mov.u32 	%r6, 0;
	st.global.u32 	[%rd1], %r6;

BB17_4:
	ret;
}

	// .globl	equalTo
.visible .entry equalTo(
	.param .f32 equalTo_param_0,
	.param .u64 equalTo_param_1,
	.param .u32 equalTo_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<5>;


	ld.param.f32 	%f1, [equalTo_param_0];
	ld.param.u64 	%rd2, [equalTo_param_1];
	ld.param.u32 	%r2, [equalTo_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB18_4;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd1];
	setp.eq.f32	%p2, %f2, %f1;
	@%p2 bra 	BB18_3;
	bra.uni 	BB18_2;

BB18_3:
	mov.u32 	%r7, 1065353216;
	st.global.u32 	[%rd1], %r7;
	bra.uni 	BB18_4;

BB18_2:
	mov.u32 	%r6, 0;
	st.global.u32 	[%rd1], %r6;

BB18_4:
	ret;
}

	// .globl	addLogs
.visible .entry addLogs(
	.param .u64 addLogs_param_0,
	.param .u64 addLogs_param_1,
	.param .u32 addLogs_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<68>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd2, [addLogs_param_0];
	ld.param.u64 	%rd3, [addLogs_param_1];
	ld.param.u32 	%r8, [addLogs_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	mul.wide.u32 	%rd4, %r3, 4;
	mov.u64 	%rd5, chunk;
	add.s64 	%rd1, %rd5, %rd4;
	setp.ge.s32	%p1, %r4, %r8;
	@%p1 bra 	BB19_2;

	cvta.to.global.u64 	%rd6, %rd3;
	mov.u32 	%r9, %ctaid.y;
	mad.lo.s32 	%r10, %r9, %r8, %r4;
	mul.wide.u32 	%rd7, %r10, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f6, [%rd8];
	st.shared.f32 	[%rd1], %f6;

BB19_2:
	bar.sync 	0;
	shr.u32 	%r20, %r1, 1;
	setp.eq.s32	%p2, %r20, 0;
	@%p2 bra 	BB19_8;

BB19_3:
	add.s32 	%r11, %r20, %r4;
	setp.lt.s32	%p3, %r11, %r8;
	setp.lt.u32	%p4, %r3, %r20;
	and.pred  	%p5, %p4, %p3;
	@!%p5 bra 	BB19_7;
	bra.uni 	BB19_4;

BB19_4:
	ld.shared.f32 	%f11, [%rd1];
	add.s32 	%r12, %r20, %r3;
	mul.wide.u32 	%rd9, %r12, 4;
	add.s64 	%rd11, %rd5, %rd9;
	ld.shared.f32 	%f12, [%rd11];
	max.f32 	%f1, %f11, %f12;
	sub.f32 	%f13, %f11, %f1;
	mul.f32 	%f14, %f13, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f15, %f14;
	mov.f32 	%f16, 0fBF317200;
	fma.rn.f32 	%f17, %f15, %f16, %f13;
	mov.f32 	%f18, 0fB5BFBE8E;
	fma.rn.f32 	%f19, %f15, %f18, %f17;
	mul.f32 	%f8, %f19, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f7,%f8;
	// inline asm
	add.f32 	%f20, %f15, 0f00000000;
	ex2.approx.f32 	%f21, %f20;
	mul.f32 	%f22, %f7, %f21;
	setp.lt.f32	%p6, %f13, 0fC2D20000;
	selp.f32	%f23, 0f00000000, %f22, %p6;
	setp.gt.f32	%p7, %f13, 0f42D20000;
	selp.f32	%f24, 0f7F800000, %f23, %p7;
	sub.f32 	%f25, %f12, %f1;
	mul.f32 	%f26, %f25, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f27, %f26;
	fma.rn.f32 	%f28, %f27, %f16, %f25;
	fma.rn.f32 	%f29, %f27, %f18, %f28;
	mul.f32 	%f10, %f29, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f9,%f10;
	// inline asm
	add.f32 	%f30, %f27, 0f00000000;
	ex2.approx.f32 	%f31, %f30;
	mul.f32 	%f32, %f9, %f31;
	setp.lt.f32	%p8, %f25, 0fC2D20000;
	selp.f32	%f33, 0f00000000, %f32, %p8;
	setp.gt.f32	%p9, %f25, 0f42D20000;
	selp.f32	%f34, 0f7F800000, %f33, %p9;
	add.f32 	%f35, %f24, %f34;
	setp.lt.f32	%p10, %f35, 0f00800000;
	mul.f32 	%f36, %f35, 0f4B000000;
	selp.f32	%f2, %f36, %f35, %p10;
	selp.f32	%f37, 0fC1B80000, 0f00000000, %p10;
	mov.b32 	 %r13, %f2;
	add.s32 	%r14, %r13, -1059760811;
	and.b32  	%r15, %r14, -8388608;
	sub.s32 	%r16, %r13, %r15;
	mov.b32 	 %f38, %r16;
	cvt.rn.f32.s32	%f39, %r15;
	mov.f32 	%f40, 0f34000000;
	fma.rn.f32 	%f41, %f39, %f40, %f37;
	add.f32 	%f42, %f38, 0fBF800000;
	mov.f32 	%f43, 0f3E1039F6;
	mov.f32 	%f44, 0fBE055027;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0fBDF8CDCC;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3E0F2955;
	fma.rn.f32 	%f49, %f47, %f42, %f48;
	mov.f32 	%f50, 0fBE2AD8B9;
	fma.rn.f32 	%f51, %f49, %f42, %f50;
	mov.f32 	%f52, 0f3E4CED0B;
	fma.rn.f32 	%f53, %f51, %f42, %f52;
	mov.f32 	%f54, 0fBE7FFF22;
	fma.rn.f32 	%f55, %f53, %f42, %f54;
	mov.f32 	%f56, 0f3EAAAA78;
	fma.rn.f32 	%f57, %f55, %f42, %f56;
	mov.f32 	%f58, 0fBF000000;
	fma.rn.f32 	%f59, %f57, %f42, %f58;
	mul.f32 	%f60, %f59, %f42;
	fma.rn.f32 	%f61, %f60, %f42, %f42;
	mov.f32 	%f62, 0f3F317218;
	fma.rn.f32 	%f67, %f41, %f62, %f61;
	setp.lt.u32	%p11, %r13, 2139095040;
	@%p11 bra 	BB19_6;

	mov.f32 	%f63, 0f7F800000;
	fma.rn.f32 	%f67, %f2, %f63, %f63;

BB19_6:
	setp.eq.f32	%p12, %f2, 0f00000000;
	selp.f32	%f64, 0fFF800000, %f67, %p12;
	add.f32 	%f65, %f1, %f64;
	st.shared.f32 	[%rd1], %f65;

BB19_7:
	bar.sync 	0;
	shr.s32 	%r20, %r20, 1;
	setp.gt.s32	%p13, %r20, 0;
	@%p13 bra 	BB19_3;

BB19_8:
	setp.ne.s32	%p14, %r3, 0;
	@%p14 bra 	BB19_10;

	cvta.to.global.u64 	%rd12, %rd2;
	ld.shared.f32 	%f66, [chunk];
	mov.u32 	%r17, %nctaid.x;
	mov.u32 	%r18, %ctaid.y;
	mad.lo.s32 	%r19, %r17, %r18, %r2;
	mul.wide.u32 	%rd13, %r19, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f32 	[%rd14], %f66;

BB19_10:
	ret;
}

	// .globl	powScaler
.visible .entry powScaler(
	.param .f32 powScaler_param_0,
	.param .u64 powScaler_param_1,
	.param .u32 powScaler_param_2
)
{
	.reg .pred 	%p<30>;
	.reg .f32 	%f<103>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<5>;


	ld.param.f32 	%f17, [powScaler_param_0];
	ld.param.u64 	%rd2, [powScaler_param_1];
	ld.param.u32 	%r2, [powScaler_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p2, %r1, %r2;
	@%p2 bra 	BB20_15;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	mul.f32 	%f22, %f17, 0f3F000000;
	cvt.rzi.f32.f32	%f23, %f22;
	fma.rn.f32 	%f24, %f23, 0fC0000000, %f17;
	abs.f32 	%f1, %f24;
	ld.global.f32 	%f2, [%rd1];
	abs.f32 	%f3, %f2;
	setp.lt.f32	%p3, %f3, 0f00800000;
	mul.f32 	%f25, %f3, 0f4B800000;
	selp.f32	%f26, 0fC3170000, 0fC2FE0000, %p3;
	selp.f32	%f27, %f25, %f3, %p3;
	mov.b32 	 %r6, %f27;
	and.b32  	%r7, %r6, 8388607;
	or.b32  	%r8, %r7, 1065353216;
	mov.b32 	 %f28, %r8;
	shr.u32 	%r9, %r6, 23;
	cvt.rn.f32.u32	%f29, %r9;
	add.f32 	%f30, %f26, %f29;
	setp.gt.f32	%p4, %f28, 0f3FB504F3;
	mul.f32 	%f31, %f28, 0f3F000000;
	add.f32 	%f32, %f30, 0f3F800000;
	selp.f32	%f33, %f31, %f28, %p4;
	selp.f32	%f34, %f32, %f30, %p4;
	add.f32 	%f35, %f33, 0fBF800000;
	add.f32 	%f19, %f33, 0f3F800000;
	// inline asm
	rcp.approx.ftz.f32 %f18,%f19;
	// inline asm
	add.f32 	%f36, %f35, %f35;
	mul.f32 	%f37, %f18, %f36;
	mul.f32 	%f38, %f37, %f37;
	mov.f32 	%f39, 0f3C4CAF63;
	mov.f32 	%f40, 0f3B18F0FE;
	fma.rn.f32 	%f41, %f40, %f38, %f39;
	mov.f32 	%f42, 0f3DAAAABD;
	fma.rn.f32 	%f43, %f41, %f38, %f42;
	mul.rn.f32 	%f44, %f43, %f38;
	mul.rn.f32 	%f45, %f44, %f37;
	sub.f32 	%f46, %f35, %f37;
	neg.f32 	%f47, %f37;
	add.f32 	%f48, %f46, %f46;
	fma.rn.f32 	%f49, %f47, %f35, %f48;
	mul.rn.f32 	%f50, %f18, %f49;
	add.f32 	%f51, %f45, %f37;
	sub.f32 	%f52, %f37, %f51;
	add.f32 	%f53, %f45, %f52;
	add.f32 	%f54, %f50, %f53;
	add.f32 	%f55, %f51, %f54;
	sub.f32 	%f56, %f51, %f55;
	add.f32 	%f57, %f54, %f56;
	mov.f32 	%f58, 0f3F317200;
	mul.rn.f32 	%f59, %f34, %f58;
	mov.f32 	%f60, 0f35BFBE8E;
	mul.rn.f32 	%f61, %f34, %f60;
	add.f32 	%f62, %f59, %f55;
	sub.f32 	%f63, %f59, %f62;
	add.f32 	%f64, %f55, %f63;
	add.f32 	%f65, %f57, %f64;
	add.f32 	%f66, %f61, %f65;
	add.f32 	%f67, %f62, %f66;
	sub.f32 	%f68, %f62, %f67;
	add.f32 	%f69, %f66, %f68;
	abs.f32 	%f4, %f17;
	setp.gt.f32	%p5, %f4, 0f77F684DF;
	mul.f32 	%f70, %f17, 0f39000000;
	selp.f32	%f71, %f70, %f17, %p5;
	mul.rn.f32 	%f72, %f71, %f67;
	neg.f32 	%f73, %f72;
	fma.rn.f32 	%f74, %f71, %f67, %f73;
	fma.rn.f32 	%f75, %f71, %f69, %f74;
	mov.f32 	%f76, 0f00000000;
	fma.rn.f32 	%f77, %f76, %f67, %f75;
	add.rn.f32 	%f78, %f72, %f77;
	neg.f32 	%f79, %f78;
	add.rn.f32 	%f80, %f72, %f79;
	add.rn.f32 	%f81, %f80, %f77;
	mov.b32 	 %r10, %f78;
	setp.eq.s32	%p6, %r10, 1118925336;
	add.s32 	%r11, %r10, -1;
	mov.b32 	 %f82, %r11;
	add.f32 	%f83, %f81, 0f37000000;
	selp.f32	%f84, %f82, %f78, %p6;
	selp.f32	%f5, %f83, %f81, %p6;
	mul.f32 	%f85, %f84, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f86, %f85;
	mov.f32 	%f87, 0fBF317200;
	fma.rn.f32 	%f88, %f86, %f87, %f84;
	mov.f32 	%f89, 0fB5BFBE8E;
	fma.rn.f32 	%f90, %f86, %f89, %f88;
	mul.f32 	%f21, %f90, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f20,%f21;
	// inline asm
	add.f32 	%f91, %f86, 0f00000000;
	ex2.approx.f32 	%f92, %f91;
	mul.f32 	%f93, %f20, %f92;
	setp.lt.f32	%p7, %f84, 0fC2D20000;
	selp.f32	%f94, 0f00000000, %f93, %p7;
	setp.gt.f32	%p8, %f84, 0f42D20000;
	selp.f32	%f101, 0f7F800000, %f94, %p8;
	setp.eq.f32	%p9, %f101, 0f7F800000;
	@%p9 bra 	BB20_3;

	fma.rn.f32 	%f101, %f101, %f5, %f101;

BB20_3:
	setp.lt.f32	%p10, %f2, 0f00000000;
	setp.eq.f32	%p11, %f1, 0f3F800000;
	and.pred  	%p1, %p10, %p11;
	mov.b32 	 %r12, %f101;
	xor.b32  	%r13, %r12, -2147483648;
	mov.b32 	 %f95, %r13;
	selp.f32	%f102, %f95, %f101, %p1;
	setp.eq.f32	%p12, %f2, 0f00000000;
	@%p12 bra 	BB20_6;
	bra.uni 	BB20_4;

BB20_6:
	add.f32 	%f97, %f2, %f2;
	mov.b32 	 %r14, %f97;
	selp.b32	%r15, %r14, 0, %p11;
	or.b32  	%r16, %r15, 2139095040;
	setp.lt.f32	%p16, %f17, 0f00000000;
	selp.b32	%r17, %r16, %r15, %p16;
	mov.b32 	 %f102, %r17;
	bra.uni 	BB20_7;

BB20_4:
	setp.geu.f32	%p13, %f2, 0f00000000;
	@%p13 bra 	BB20_7;

	cvt.rzi.f32.f32	%f96, %f17;
	setp.neu.f32	%p14, %f96, %f17;
	selp.f32	%f102, 0f7FFFFFFF, %f102, %p14;

BB20_7:
	add.f32 	%f98, %f3, %f4;
	mov.b32 	 %r18, %f98;
	setp.lt.s32	%p17, %r18, 2139095040;
	@%p17 bra 	BB20_14;

	setp.gtu.f32	%p18, %f3, 0f7F800000;
	setp.gtu.f32	%p19, %f4, 0f7F800000;
	or.pred  	%p20, %p18, %p19;
	@%p20 bra 	BB20_13;
	bra.uni 	BB20_9;

BB20_13:
	add.f32 	%f102, %f2, %f17;
	bra.uni 	BB20_14;

BB20_9:
	setp.eq.f32	%p21, %f4, 0f7F800000;
	@%p21 bra 	BB20_12;
	bra.uni 	BB20_10;

BB20_12:
	setp.gt.f32	%p24, %f3, 0f3F800000;
	selp.b32	%r22, 2139095040, 0, %p24;
	xor.b32  	%r23, %r22, 2139095040;
	setp.lt.f32	%p25, %f17, 0f00000000;
	selp.b32	%r24, %r23, %r22, %p25;
	mov.b32 	 %f99, %r24;
	setp.eq.f32	%p26, %f2, 0fBF800000;
	selp.f32	%f102, 0f3F800000, %f99, %p26;
	bra.uni 	BB20_14;

BB20_10:
	setp.neu.f32	%p22, %f3, 0f7F800000;
	@%p22 bra 	BB20_14;

	setp.ltu.f32	%p23, %f17, 0f00000000;
	selp.b32	%r19, 0, 2139095040, %p23;
	or.b32  	%r20, %r19, -2147483648;
	selp.b32	%r21, %r20, %r19, %p1;
	mov.b32 	 %f102, %r21;

BB20_14:
	setp.eq.f32	%p27, %f17, 0f00000000;
	setp.eq.f32	%p28, %f2, 0f3F800000;
	or.pred  	%p29, %p28, %p27;
	selp.f32	%f100, 0f3F800000, %f102, %p29;
	st.global.f32 	[%rd1], %f100;

BB20_15:
	ret;
}

	// .globl	mapForward
.visible .entry mapForward(
	.param .u64 mapForward_param_0,
	.param .u64 mapForward_param_1,
	.param .u64 mapForward_param_2,
	.param .u32 mapForward_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [mapForward_param_0];
	ld.param.u64 	%rd2, [mapForward_param_1];
	ld.param.u64 	%rd3, [mapForward_param_2];
	ld.param.u32 	%r2, [mapForward_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB21_2;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u32 	%r6, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.s32 	%rd8, %r6, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.f32 	%f1, [%rd9];
	cvta.to.global.u64 	%rd10, %rd1;
	add.s64 	%rd11, %rd10, %rd5;
	st.global.f32 	[%rd11], %f1;

BB21_2:
	ret;
}

	// .globl	mapBackward
.visible .entry mapBackward(
	.param .u64 mapBackward_param_0,
	.param .u64 mapBackward_param_1,
	.param .u64 mapBackward_param_2,
	.param .u32 mapBackward_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [mapBackward_param_0];
	ld.param.u64 	%rd2, [mapBackward_param_1];
	ld.param.u64 	%rd3, [mapBackward_param_2];
	ld.param.u32 	%r2, [mapBackward_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB22_2;

	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u32 	%r6, [%rd6];
	cvta.to.global.u64 	%rd7, %rd1;
	mul.wide.s32 	%rd8, %r6, 4;
	add.s64 	%rd9, %rd7, %rd8;
	cvta.to.global.u64 	%rd10, %rd2;
	add.s64 	%rd11, %rd10, %rd5;
	ld.global.f32 	%f1, [%rd11];
	atom.global.add.f32 	%f2, [%rd9], %f1;

BB22_2:
	ret;
}

	// .globl	mapMax
.visible .entry mapMax(
	.param .u64 mapMax_param_0,
	.param .u64 mapMax_param_1,
	.param .u32 mapMax_param_2,
	.param .u32 mapMax_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd4, [mapMax_param_0];
	ld.param.u64 	%rd5, [mapMax_param_1];
	ld.param.u32 	%r12, [mapMax_param_2];
	ld.param.u32 	%r11, [mapMax_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.s32	%p1, %r4, %r12;
	@%p1 bra 	BB23_5;

	mul.lo.s32 	%r5, %r4, %r11;
	mov.u32 	%r20, 0;
	setp.lt.s32	%p2, %r11, 2;
	@%p2 bra 	BB23_4;

	cvta.to.global.u64 	%rd6, %rd5;
	mul.wide.s32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f5, [%rd8];
	mul.lo.s32 	%r17, %r11, %r4;
	mul.wide.s32 	%rd9, %r17, 4;
	add.s64 	%rd10, %rd9, %rd6;
	add.s64 	%rd14, %rd10, 4;
	mov.u32 	%r20, 0;
	mov.u32 	%r19, 1;

BB23_3:
	ld.global.f32 	%f4, [%rd14];
	setp.gt.f32	%p3, %f4, %f5;
	selp.b32	%r20, %r19, %r20, %p3;
	selp.f32	%f5, %f4, %f5, %p3;
	add.s64 	%rd14, %rd14, 4;
	add.s32 	%r19, %r19, 1;
	setp.lt.s32	%p4, %r19, %r11;
	@%p4 bra 	BB23_3;

BB23_4:
	cvta.to.global.u64 	%rd11, %rd4;
	mul.wide.s32 	%rd12, %r4, 4;
	add.s64 	%rd13, %rd11, %rd12;
	add.s32 	%r18, %r20, %r5;
	st.global.u32 	[%rd13], %r18;

BB23_5:
	ret;
}


`
